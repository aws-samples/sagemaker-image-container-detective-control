{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SageMaker Training Job Image URI Restriction\n",
    "\n",
    "This example notebook can be used for testing the solution provided.\n",
    "\n",
    "In order to use this notebook, you can crate the CloudFormation Stack by providing in the `TrainingContainers` parameter the following value:\n",
    "\n",
    "**TrainingContainers**: pytorch-training:1.10-cpu-py38\n",
    "\n",
    "In this way, we are avoiding the usage of the SageMaker Container for PyTorch v1.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We are using a subset of ~20000 records of synthetic transactions, each of which is labeled as fraudulent or not fraudulent.\n",
    "We'd like to train a model based on the features of these transactions so that we can predict risky or fraudulent transactions in the future.\n",
    "\n",
    "This is a binary classification problem:\n",
    "\n",
    "* 1 - Fraud\n",
    "* 0 - No Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -rf ./data && mkdir -p ./data\n",
    "\n",
    "! aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic_credit_card_transactions/user0_credit_card_transactions.csv ./data/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the latest version of the SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install 'sagemaker' --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1/3 - Setup\n",
    "\n",
    "Here we'll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Upload Dataset in the Default Amazon S3 Bucket\n",
    "\n",
    "In order to make the data available, we are uploading the downloaded dataset into the default S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client.delete_object(Bucket=bucket_name, Key=\"sg-container-restriction/data/input\")\n",
    "\n",
    "input_data = sagemaker_session.upload_data(\n",
    "    \"./data/data.csv\", key_prefix=\"sg-container-restriction/data/input\"\n",
    ")\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Script\n",
    "\n",
    "We are creating the file `train.py` for using it in the SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import csv\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "def split_data(df):\n",
    "    try:\n",
    "        df = df[df['Is Fraud?'].notna()]\n",
    "\n",
    "        df.insert(0, 'ID', range(1, len(df) + 1))\n",
    "\n",
    "        df[\"Errors?\"].fillna('', inplace=True)\n",
    "        df['Errors?'] = df['Errors?'].map(lambda x: x.strip())\n",
    "        df[\"Errors?\"] = df[\"Errors?\"].map({\n",
    "            \"Insufficient Balance\": 0,\n",
    "            \"Technical Glitch\": 1,\n",
    "            \"Bad PIN\": 2,\n",
    "            \"Bad Expiration\": 3,\n",
    "            \"Bad Card Number\": 4,\n",
    "            \"Bad CVV\": 5,\n",
    "            \"Bad PIN,Insufficient Balance\": 6,\n",
    "            \"Bad PIN,Technical Glitch\": 7,\n",
    "            \"\": 8\n",
    "        })\n",
    "\n",
    "        df[\"Use Chip\"].fillna('', inplace=True)\n",
    "        df['Use Chip'] = df['Use Chip'].map(lambda x: x.strip())\n",
    "        df[\"Use Chip\"] = df[\"Use Chip\"].map({\n",
    "            \"Swipe Transaction\": 0,\n",
    "            \"Chip Transaction\": 1,\n",
    "            \"Online Transaction\": 2\n",
    "        })\n",
    "\n",
    "        df['Is Fraud?'] = df['Is Fraud?'].map(lambda x: x.replace(\"'\", \"\"))\n",
    "        df['Is Fraud?'] = df['Is Fraud?'].map(lambda x: x.strip())\n",
    "        df['Is Fraud?'] = df['Is Fraud?'].replace('', np.nan)\n",
    "        df['Is Fraud?'] = df['Is Fraud?'].replace(' ', np.nan)\n",
    "\n",
    "        df[\"Is Fraud?\"] = df[\"Is Fraud?\"].map({\"No\": 0, \"Yes\": 1})\n",
    "\n",
    "        df = df.rename(\n",
    "            columns={'Card': 'card', 'MCC': 'mcc', \"Errors?\": \"errors\", \"Use Chip\": \"use_chip\", \"Is Fraud?\": \"labels\"})\n",
    "\n",
    "        df = df[[\"card\", \"mcc\", \"errors\", \"use_chip\", \"labels\"]]\n",
    "        \n",
    "        train, test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "        \n",
    "        return train, test\n",
    "    except Exception as e:\n",
    "        stacktrace = traceback.format_exc()\n",
    "        logger.error(\"{}\".format(stacktrace))\n",
    "\n",
    "        raise e\n",
    "\n",
    "def prepare_data(train, test):\n",
    "    try:\n",
    "        X_train, y_train = train.iloc[:, train.columns != 'labels'], train.iloc[:, train.columns == 'labels']\n",
    "        X_test, y_test = test.iloc[:, test.columns != 'labels'], test.iloc[:, train.columns == 'labels']\n",
    "\n",
    "        y_test = y_test.astype(\"int64\")\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(X_train.values)\n",
    "        X_test = scaler.fit_transform(X_test.values)\n",
    "\n",
    "        X_train_tensor = torch.from_numpy(X_train)\n",
    "        y_train_tensor = torch.from_numpy(y_train.values.ravel()).float()\n",
    "        y_train_tensor = y_train_tensor.unsqueeze(1)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        y_test_tensor = torch.from_numpy(y_test.values.ravel()).float()\n",
    "        y_test_tensor = y_test_tensor.unsqueeze(1)\n",
    "\n",
    "        train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=args.batch_size)\n",
    "        test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "        return train_dl, test_dl\n",
    "    except Exception as e:\n",
    "        stacktrace = traceback.format_exc()\n",
    "        logger.error(\"{}\".format(stacktrace))\n",
    "\n",
    "        raise e\n",
    "\n",
    "class BinaryClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(BinaryClassifierModel, self).__init__()\n",
    "\n",
    "        self.d1 = torch.nn.Linear(shape, 32)\n",
    "        self.d2 = torch.nn.Linear(32, 64)\n",
    "        self.drop = torch.nn.Dropout(0.2)\n",
    "        self.output = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.d1(x))\n",
    "        x = torch.relu(self.d2(x))\n",
    "        x = self.drop(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--learning_rate', type=float, default=1.45e-4)\n",
    "    parser.add_argument('--batch_size', type=int, default=100)\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    data = pd.read_csv(\n",
    "        args.train + \"/data.csv\",\n",
    "        sep=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        escapechar='\\\\',\n",
    "        encoding='utf-8',\n",
    "        error_bad_lines=False\n",
    "    )\n",
    "    \n",
    "    train, test = split_data(data)\n",
    "\n",
    "    train_dl, test_dl = prepare_data(train, test)\n",
    "\n",
    "    model = BinaryClassifierModel(train.shape[1] - 1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    loss_obj = torch.nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logger.info(\"Epoch {}\".format(epoch + 1))\n",
    "\n",
    "        # Within each epoch run the subsets of data = batch sizes.\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_pred = model(xb.float()) # Forward Propagation\n",
    "\n",
    "            loss = loss_obj(y_pred, yb)  # Loss Computation\n",
    "\n",
    "            optimizer.zero_grad()  # Clearing all previous gradients, setting to zero\n",
    "            loss.backward()  # Back Propagation\n",
    "            optimizer.step()  # Updating the parameters\n",
    "\n",
    "        logger.info(\"Training Loss: {}\".format(loss.item()))\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    torch.save(model.cpu(), os.path.join(args.model_dir, \"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Global Parameters\n",
    "\n",
    "In this section, we are defining the parameters for the SageMaker Estimator. As framework-version, we use the PyTorch v1.12 and check that the SageMaker Training Job can be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_input_files_path = \"sg-container-restriction/data/input\"\n",
    "training_output_files_path = \"sg-container-restriction/models\"\n",
    "training_framework_version = \"1.12\"\n",
    "training_python_version = \"py38\"\n",
    "training_hyperparameters = {\"epochs\": 6, \"learning_rate\": 1.34e-4, \"batch_size\": 100}\n",
    "\n",
    "training_instance_count = 1\n",
    "training_instance_type = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"./train.py\",\n",
    "    framework_version=training_framework_version,\n",
    "    py_version=training_python_version,\n",
    "    output_path=\"s3://{}/{}\".format(bucket_name,\n",
    "                                    training_output_files_path),\n",
    "    hyperparameters=training_hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=training_instance_type,\n",
    "    disable_profiler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": \"s3://{}/{}\".format(\n",
    "            bucket_name,\n",
    "            training_input_files_path\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Framework Version\n",
    "\n",
    "If we change the PyTorch version to 1.10, the expected result is that the provided solution will automatically stop the SageMaker Job, since it is using a version in the provided black list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_framework_version = \"1.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"./train.py\",\n",
    "    framework_version=training_framework_version,\n",
    "    py_version=training_python_version,\n",
    "    output_path=\"s3://{}/{}\".format(bucket_name,\n",
    "                                    training_output_files_path),\n",
    "    hyperparameters=training_hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=training_instance_type,\n",
    "    disable_profiler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": \"s3://{}/{}\".format(\n",
    "            bucket_name,\n",
    "            training_input_files_path\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
